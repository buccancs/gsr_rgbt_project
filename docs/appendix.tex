\documentclass{article}
\usepackage{amsmath,graphicx, algorithm, algorithmic}
\usepackage{geometry}
\geometry{a4paper, margin=1in}
\begin{document}
    \section{Project Background}

    The GSR-RGBT project integrates Galvanic Skin Response (GSR) measurements with combined RGB (visible light) and thermal imaging to create a rich multimodal sensing platform. This cross-disciplinary system leverages physiological signals and environmental imaging, enabling new research in human-computer interaction, affective computing, and security. GSR sensors measure the electrical conductance of the skin, which correlates with sympathetic nervous system activity and emotion.


    \section{Motivation}

    Combining GSR data with RGB and thermal imaging addresses limitations of single-modality systems. For example, thermal cameras function in low-light or night conditions where RGB fails, while GSR provides internal state information that cameras cannot capture. Key motivations include:

    \begin{itemize}
        \item \textbf{Robustness:} Thermal imaging ensures visibility in poor lighting or occlusion, complementing RGB data. GSR adds internal user state for context-aware responses.
        \item \textbf{Enhanced Analysis:} Fusion of physiological and visual data enables richer analysis; for instance, a robot can adjust its behaviour not only based on scene content but also on human stress levels.
        \item \textbf{Novel Applications:} Use cases include detecting nervousness in security checkpoints, monitoring patient responses in medical settings, or adaptive gaming/environments based on player arousal.
    \end{itemize}

    The multidisciplinary nature of this project aims to push the state-of-the-art in multisensory integration, building on existing research in biosignal processing and computer vision.


    \section{Setup Instructions}

    \subsection{Hardware Requirements}

    \begin{itemize}
        \item An RGB camera (e.g., a high-resolution webcam or DSLR).
        \item A thermal imaging camera (e.g., FLIR sensor) synchronized with the RGB camera.
        \item Galvanic Skin Response (GSR) sensor hardware (e.g., biosensor wristband or electrodes) with data logging capability.
        \item A computer (PC or embedded system) with sufficient CPU/GPU capabilities to handle real-time image processing.
        \item Tripods, mounts, and cables to align and connect the cameras and sensors properly.
        \item A calibration target (like a checkerboard pattern) for aligning the RGB and thermal camera viewpoints.
    \end{itemize}

    \subsection{Software Requirements}

    \begin{itemize}
        \item \textbf{Operating System:} Linux (Ubuntu 20.04 LTS recommended) or other OS that supports necessary drivers.
        \item \textbf{Programming Language:} Python 3.8+ or C++ (depending on implementation details).
        \item \textbf{Libraries/Frameworks:}
        \begin{itemize}
            \item OpenCV (for image capture, processing, and camera calibration).
            \item NumPy and SciPy (for numerical processing).
            \item PySerial or similar (for interfacing with GSR hardware).
            \item Machine learning libraries if used (e.g., TensorFlow, PyTorch).
            \item Any drivers for the specific cameras/sensors (e.g., FLIR SDK for thermal camera).
        \end{itemize}
    \end{itemize}

    \subsection{Installation}

    \begin{enumerate}
        \item Clone the repository:
        \begin{verbatim}
git clone https://github.com/yourusername/gsr_rgbt_project.git
cd gsr_rgbt_project
        \end{verbatim}
        \item Install Python dependencies:
        \begin{verbatim}
pip install -r requirements.txt
        \end{verbatim}
        \item Install any necessary system dependencies (for example, development libraries for OpenCV):
        \begin{verbatim}
sudo apt-get update
sudo apt-get install build-essential cmake libgtk-3-dev libboost-all-dev
        \end{verbatim}
        \item Set up camera drivers or SDKs as per manufacturer instructions (e.g., install FLIR SDK for thermal camera).
        \item (Optional) Compile any C++ modules or examples if provided:
        \begin{verbatim}
cd cpp_modules
mkdir build && cd build
cmake ..
make
        \end{verbatim}
    \end{enumerate}


    \section{Architecture Design}

    The system architecture follows a modular design where each sensor input is captured, synchronized, and processed through dedicated components. Key elements include:

    \begin{itemize}
        \item \textbf{Sensor Modules:} Separate software components handle data acquisition from each device. The RGB and thermal cameras are typically accessed via OpenCV interfaces, while the GSR sensor data is read through a serial or Bluetooth connection.
        \item \textbf{Synchronization Layer:} A time synchronization mechanism tags each sensor reading with a common timestamp. This may use hardware triggers (if available) or software time-stamping to align frames from RGB and thermal cameras and the corresponding GSR data~\cite{RefSync}.
        \item \textbf{Processing Pipeline:} A real-time pipeline processes the synchronized data. Steps may include image preprocessing (e.g., undistortion, alignment between RGB and thermal frames), signal processing for GSR, and feature extraction.
        \item \textbf{Data Fusion Module:} Combines features from visual data (e.g., object detection results, pixel intensities) with GSR metrics. This can be a rule-based system or machine learning model that takes both modalities as input.
        \item \textbf{Application Logic:} Depending on the application (e.g., alert generation, logging, user feedback), the fused data is used to make decisions or store results.
    \end{itemize}

    The overall data flow can be visualized as follows: an RGB image and a thermal image are captured in parallel and preprocessed, while GSR values are read asynchronously. These streams are synchronized by timestamp, then passed to the data fusion and analysis stage. For example, if $(u,v)$ are pixel coordinates in the RGB image, a transformation using a homography $H$ aligns the thermal image via:
    \[
        \begin{bmatrix}
            u' \\ v' \\ 1
        \end{bmatrix} = H \begin{bmatrix}
                              u \\ v \\ 1
        \end{bmatrix},
    \]
    where $H$ is a $3\times 3$ homography matrix estimated during calibration.

    For clarity, a pseudocode representation of the main processing loop is:
    \begin{verbatim}
while system_running:
    rgb_frame = capture_rgb_camera()
    thermal_frame = capture_thermal_camera()
    gsr_value = read_gsr_sensor()
    timestamp = current_time()
    # Preprocess images
    rgb_processed = preprocess_image(rgb_frame)
    thermal_processed = preprocess_image(thermal_frame)
    # Synchronize and fuse data
    fused_features = fuse_data(rgb_processed, thermal_processed, gsr_value, timestamp)
    # Perform analysis or decision making
    result = analyze(fused_features)
    # Output results
    display_or_log(result)
    \end{verbatim}

    This modular design makes it easier to debug individual components and allows parallel processing of different sensor streams.


    \section{Implementation Guide}

    \subsection{Data Acquisition}

    Each sensor is handled by a dedicated acquisition script or module. For example, the RGB camera capture might use OpenCV’s \texttt{VideoCapture}, and the thermal camera may use a proprietary SDK or video stream. The GSR sensor is typically read via a serial interface. Ensure that all devices are initialized before the main loop begins.

    \begin{verbatim}
# Initialize sensors
rgb_cam = cv2.VideoCapture(0)
thermal_cam = ThermalCameraSDK()  # pseudo-code for thermal camera initialization
gsr_sensor = serial.Serial('/dev/ttyUSB0', 9600)
    \end{verbatim}

    \subsection{Preprocessing Pipeline}

    Preprocessing steps include:

    \begin{itemize}
        \item \textbf{Image Undistortion:} Correct lens distortion using calibration (e.g., OpenCV’s \texttt{undistort}).
        \item \textbf{Alignment:} Align the thermal image to the RGB frame. Detect a calibration pattern (checkerboard) in both images and compute a homography or affine transform.
        \item \textbf{Filtering:} Apply noise reduction or normalization. For GSR signals, apply smoothing (e.g., a moving average filter) to remove high-frequency noise.
    \end{itemize}

    For example, in code:

    \begin{verbatim}
# Assume camera matrices and distortion coefficients are known
rgb_undistorted = cv2.undistort(rgb_frame, rgb_camera_matrix, rgb_dist_coeffs)
thermal_undistorted = cv2.undistort(thermal_frame, thermal_camera_matrix, thermal_dist_coeffs)
# Compute transform from thermal to RGB using calibration pattern points
H, _ = cv2.findHomography(thermal_points, rgb_points)
thermal_aligned = cv2.warpPerspective(thermal_undistorted, H, (rgb_frame.shape[1], rgb_frame.shape[0]))
    \end{verbatim}

    \subsection{Feature Extraction and Analysis}

    Features are extracted depending on the application:
    \begin{itemize}
        \item \textbf{Image Features:} Detected objects (using a model like YOLO), heat blobs from thermal images, edges, or texture descriptors.
        \item \textbf{Signal Features:} GSR raw values, peaks, or derived metrics (phasic/tonic components).
    \end{itemize}

    These are combined into a feature vector for further processing. For example:

    \begin{verbatim}
# Example: combine features
object_confidence = detect_object(rgb_undistorted)
thermal_max_temp = thermal_aligned.max()
gsr_level = compute_gsr_metric(gsr_raw)
feature_vector = [object_confidence, thermal_max_temp, gsr_level]
    \end{verbatim}

    \subsection{Code Organization}

    The repository is organized into directories such as:
    \begin{itemize}
        \item \texttt{src/}: Source code (Python scripts, C++ files).
        \item \texttt{data/}: Sample datasets or calibration images.
        \item \texttt{models/}: Pre-trained models or weights.
        \item \texttt{docs/}: Documentation and design diagrams.
        \item \texttt{experiments/}: Scripts for testing and validation.
    \end{itemize}
    Code should be modular, with clear functions and docstrings, following best practices.


    \section{Validation Procedures}

    Validation ensures that the system works correctly. Procedures include:
    \begin{itemize}
        \item \textbf{Unit Testing:} Test individual modules (e.g., camera interface, GSR reading) in isolation using frameworks like \texttt{pytest}.
        \item \textbf{Calibration Verification:} After calibration, verify that points in the RGB and thermal images align correctly. Check reprojection error as a metric.
        \item \textbf{Functional Testing:} Run the full pipeline on known inputs. For example, capture a sequence with known patterns and stimuli and verify outputs.
        \item \textbf{Ground Truth Comparison:} Compare system outputs to ground truth. Use a labeled dataset to measure accuracy (e.g., precision/recall for detection).
        \item \textbf{Stress Testing:} Test under varied conditions (lighting, temperature, stressors) to check robustness.
    \end{itemize}
    Detailed logs of timestamps and sensor readings should be kept for analysis.


    \section{Benchmarking}

    Benchmarking focuses on performance metrics:
    \begin{itemize}
        \item \textbf{Frame Rate (FPS):} Number of frames processed per second. Real-time systems often target 30+ FPS~\cite{RefFPS}.
        \item \textbf{Latency:} Delay from data capture to output. Aim for low latency (e.g., <100~ms).
        \item \textbf{CPU/GPU Usage:} Monitor resource utilization. High usage may require optimization.
        \item \textbf{Memory Usage:} Ensure the system runs within available memory.
        \item \textbf{Power Consumption:} Important for mobile platforms, as it affects battery life.
    \end{itemize}
    Report the hardware specifications when presenting benchmarks.


    \section{Real-Time Pipeline}

    To achieve real-time performance, design the pipeline with concurrency and efficiency:
    \begin{itemize}
        \item \textbf{Asynchronous Capture and Processing:} Run capture and processing in parallel (e.g., separate threads). Use thread-safe queues for frame buffering.
        \item \textbf{Buffering:} Use circular buffers or queues to store incoming frames without loss.
        \item \textbf{Efficient Algorithms:} Use optimised libraries and reduce computation (e.g., process every \textsubscript{n}th frame if needed).
        \item \textbf{Resource Management:} Assign threads to CPU cores or use GPU acceleration. Profile to find bottlenecks.
    \end{itemize}
    A pseudocode example for a multithreaded pipeline is shown below.

    \begin{algorithm}
        \caption{Multithreaded capture and processing}
        \begin{algorithmic}[1]
            \WHILE{running}
            \STATE frame = capture\_rgb()
            \STATE enqueue(frame)
            \STATE \textbf{if} queue is not empty \textbf{then}
            \STATE \quad frame = dequeue()
            \STATE \quad process\_frame(frame)  \COMMENT{includes thermal and GSR fusion}
            \ENDWHILE
        \end{algorithmic}
    \end{algorithm}

    Use profiling and hardware-specific optimisation (e.g., CUDA) to improve latency further.


    \section{Data Collection Forms}

    Systematic data collection is crucial. Record:
    \begin{itemize}
        \item \textbf{Experimental Parameters:} Date/time, location, subject ID, lighting conditions, etc.
        \item \textbf{Sensor Data:} For each timestamp, log sensor readings (e.g., GSR value, frame number, detected features).
        \item \textbf{Ground Truth Labels:} Mark events or stimuli in sync with sensor data.
        \item \textbf{Metadata:} Notes on anomalies, calibration settings, environmental conditions.
    \end{itemize}
    Store logs in CSV or spreadsheet format. For example:
    \begin{verbatim}
timestamp, rgb_frame_id, thermal_frame_id, gsr_value, detected_object, temperature_max, notes
    \end{verbatim}
    Use consistent units and time reference for later analysis.


    \section{Troubleshooting}

    Common issues include:
    \begin{itemize}
        \item \textbf{Camera Errors:} Ensure cameras are connected and drivers are installed. Check device indices or SDK initialization.
        \item \textbf{Synchronization Drift:} Use hardware triggers or synchronize clocks. Log timestamps to diagnose misalignment.
        \item \textbf{Calibration Misalignment:} Recalibrate with enough samples. Verify checkerboard detection in both cameras.
        \item \textbf{GSR Signal Issues:} Check sensor contact and connectivity. Filter noise and verify serial communication settings.
        \item \textbf{Performance Bottleneck:} Profile the system. Optimise slow parts (e.g., model inference or disk I/O).
        \item \textbf{Unexpected Crashes:} Run modules in isolation to find the error source. Handle exceptions and check memory usage.
    \end{itemize}
    Keep a log of all errors and solutions for future reference.


    \section{Academic Publishing Advice}

    When writing a paper or thesis on this project:
    \begin{itemize}
        \item \textbf{Clear Motivation and Contributions:} Emphasise the novelty of combining GSR with RGB and thermal sensors.
        \item \textbf{Literature Review:} Cite relevant work on multimodal sensing and context-aware systems.
        \item \textbf{Clarity of Presentation:} Include diagrams of the system architecture and example results. Use meaningful captions.
        \item \textbf{Methodology Detail:} Describe data collection and algorithms in detail~\cite{RefMethodology} (refer to earlier sections).
        \item \textbf{Evaluation:} Present quantitative and qualitative results. Discuss limitations.
        \item \textbf{Writing Quality:} Follow the target venue’s style. Get feedback from peers. Ensure thorough proofreading and citation of sources.
    \end{itemize}
    Provide links to code and data if allowed for reproducibility.


    \section{Future Work}

    Potential future directions include:
    \begin{itemize}
        \item \textbf{Advanced Fusion:} Develop neural models that jointly learn from images and GSR data.
        \item \textbf{Additional Sensors:} Incorporate other modalities (e.g., depth cameras, audio) for richer context.
        \item \textbf{Embedded Systems:} Optimise the pipeline for embedded or mobile platforms.
        \item \textbf{Longitudinal Studies:} Use the system for extended user studies (e.g., stress monitoring).
        \item \textbf{Real-World Deployment:} Collaborate with domain experts to apply the system in healthcare, security, etc.
    \end{itemize}
    These extensions can build upon the project to enhance scalability and applicability.

\bibliography{references}
\end{document}
